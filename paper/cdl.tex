\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage[noabbrev]{cleveref}
\usepackage{url}
\usepackage{brian}
\usepackage{booktabs}
\usepackage{graphicx}
% \usepackage{microtype}
\usepackage[sort]{natbib}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09


\title{Efficient convolutional coding and dictionary learning}


\author{
Brian McFee\\
Center for Jazz Studies\\
Columbia University\\
New York, NY 10027 \\
\texttt{brm2132@columbia.edu}
\And
Daniel P.W. Ellis\\
Department of Electrical Engineering\\
Columbia University\\
New York, NY 10027\\
\texttt{dpwe@columbia.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

% \nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
We make convolutional coding faster.
\end{abstract}

\section{Introduction}
\cite{mairal2010}
% TODO:   2013-05-24 18:26:56 by Brian McFee <brm2132@columbia.edu>
% why convolutional coding?
%   shift-invariance, robustness against windowing/offset effects
%   fixed codebooks tend to be overcomplete, learn the same basis at multiple shifts
%       which is wasteful
%   convolutional coding lets us use a smaller codebook
%   positions of activations can be semantically meaningful, eg in music transcription
%       cite klapuri2012


\subsection{Related work}
\cite{grosse2007}

\subsection{Preliminaries}
$\F:\R^d\rightarrow\C^d$ will denote the discrete Fourier transform, and $*$ will denote
convolution.
Time-domain signals will be represented by lower-case letters, \eg, $x \in \R^d$, while
frequency-domain representations will be denoted by capitals, $\F\{x\} = X \in \C^d$.
For a vector $u$, $\diag(u)$ will denote the square, diagonal matrix such that 
${\diag(u)}_{ii} = u_i$.  Unless otherwise stated, $\|\cdot\|$ will indicate the
$\ell_2$-norm.

\section{Convolutional coding}
\label{sec:convcode}
Given a set of $m$ codewords ${\{d_k\}}_{k=1}^m \subset \R^d$ and a signal $x\in\R^d$, 
we seek a set of activation functions ${\{a_k\}}_{k=1}^m\subset\R^d$ which
can be used to reconstruct $x$ by convolving with the codewords:
\begin{align}
x &\approx \sum_{k=1}^m d_k * a_k.\label{eq:convapprox}
\end{align}
Because this parameterization increases the dimensionality of the representation from $d$ 
to $md$, we also desire that each activation function $a_k$ be sparse.  
Formally, this is accomplished by solving the following ``shift-invariant sparse
coding'' (SISC) optimization problem:
\begin{align}
\text{(SISC)}\quad\quad a(x) &\defeq \argmin_{\{a_k\}} \frac{1}{2} \left\|x - \sum_{k=1}^m d_k * a_k\right\|^2 +
\omega(\{a_k\}), \label{eq:sisc}
\end{align}
where $\omega$ is a convex function subsuming any sparsity-promoting regularization 
(\eg, the $\ell_1$-norm) or additional constraints (\eg, non-negativity of activations).

Because convolution is a linear operation, \eqref{eq:sisc} is convex in $a_k$.  
However, direct optimization is difficult.\bn{MORE}

\subsection{Coding in the frequency domain}
By applying Parseval's theorem and the convolution theorem,\footnote{We assume circular
convolutions throughout this article. However, linear convolutions can be accommodated
in the usual way by appropriate padding and truncation. Similarly, for simplicity, all 
convolutions are assumed to be one-dimensional, but the formulation trivially
generalizes to the multi-dimensional case.} we can equivalently express the 
reconstruction term of \eqref{eq:sisc} in the frequency domain:\footnote{To ease
presentation, we assume the symmetrically normalized form of the Fourier transform, 
so that $\|x\| = \|\F\{x\}\|$. In general, a normalization constant may be included
explicitly, or absorbed into the regularization term $\Omega$.}
\begin{align}
\left\|x - \sum_k d_k * a_k\right\|^2 &= \left\|\F\left\{x - \sum_k d_k *
a_k\right\}\right\|^2
= \left\| X - \sum_k D_k \circ A_k\right\|^2,\label{eq:fourier}
\end{align}
where the last equality follows by linearity of the Fourier transform, and $\circ$
denotes the Hadamard product: ${[u\circ v]}_i \defeq u_i v_i$.

While the right-hand side of \eqref{eq:fourier} nearly resembles a standard
least-squares objective, it can be put into a more familiar form via the following
lemma.
\begin{lemma}
Let $\{D_1, D_2, \dots, D_m\}, \{A_1, A_2, \dots, A_m\} \subset \C^d$.  
Then there exist ${\widehat{D} \in \C^{d\times dm}}$ and ${\vec{A} \in \C^{dm}}$ such that 
${\widehat{D}\vec{A} = \sum_{k=1}^m D_k \circ A_k}$.\label{lemma:hadamard}
\end{lemma}
\begin{proof}
$D_k$ and $A_k$ can be rearranged as follows:
\begin{align*}
\widehat{D} &\defeq \left[\diag(D_1),\; \diag(D_2),\; \dots,\; \diag(D_m)\right] \in \C^{d\times dm}\\
\vec{A} &\defeq \left[A_1\trans, A_2\trans, \dots,
A_m\trans\right]\trans \in \C^{dm}\\
\Rightarrow \quad {\left[\widehat{D}\vec{A}\right]}_i &= \sum_\ell \widehat{D}_{i\ell}
\vec{A}_\ell = \sum_k {[D_k]}_i {[A_k]}_i = \sum_k
{[D_k \circ A_k]}_i.
\end{align*}
\end{proof}

\Cref{lemma:hadamard} allows us to interchange the Hadamard product in
\eqref{eq:fourier} with a dot-product, so that 
$\left\|x - \sum_k d_k * a_k\right\|^2 =  \left\|X - \widehat{D}\vec{A}\right\|^2$,
but we are left with a least-squares objective function parameterized by the complex-valued 
vector $\vec{A}$. To facilitate the use of standard optimization techniques to solve for
$\vec{A}$, we separate the real and imaginary components as follows:
\begin{equation}
X \mapsto \left[\begin{array}{r}\Re X \\ \Im X\end{array}\right] \in \R^{2d},
\quad\quad \widehat{D} \mapsto \left[\begin{array}{rr}\Re \widehat{D} & - \Im \widehat{D}\\
\Im \widehat{D} & \Re \widehat{D}\end{array}\right] \in \R^{2d\times 2dm},
\quad\quad \vec{A} \mapsto \left[\begin{array}{r}\Re \vec{A} \\ \Im \vec{A}\end{array}\right]
\in \R^{2dm}.\label{eq:realized}
\end{equation}
With a slight abuse of notation, we can equivalently express \eqref{eq:sisc} as
\begin{equation}
\min_{A \in \R^{2dm}} \left\|X - \widehat{D}A\right\|^2 + \Omega(A),\label{eq:realvalued}
\end{equation}
where $A_k$ corresponds to the sub-vector of $A$ associated with the $k$th codeword, and 
$\Omega$ is a frequency-domain parameterization of the regularization function $\omega$.
Given an optimal $A$, each activation function $a_k$ can be 
recovered by the inverse Fourier transform of the combined real and imaginary components
corresponding to $A_k$.

\subsection{ADMM solver}
\label{sec:solver}
At a first glance, \eqref{eq:realvalued} may seem no easier to solve than the original
formulation of \eqref{eq:sisc}, especially since in many cases, $\Omega$ may be most
naturally parameterized in the time domain.  We therefore apply the alternating
direction method of multipliers (ADMM) technique to separate the regularization and
reconstruction terms~\cite{boyd2011}.  This leads to the following equivalent
formulation:
\begin{equation}
\min_{A, Z} \frac{1}{2}\left\|X - \widehat{D}A\right\|^2 + \Omega(Z)
\quad\quad \suchthat\quad A - Z = 0.\label{eq:admm}
\end{equation}

Forming the (scaled-form) augmented Lagrangian for \eqref{eq:admm} with scaling
parameter $\rho > 0 $ yields
\[
\Ell_\rho(A,Z,W) = \frac{1}{2}\left\|X - \widehat{D}A\right\|^2 + \Omega(Z) +
\frac{\rho}{2}\|A - Z + W\|^2,
\]
where $W$ is a vector of (scaled) Lagrange multipliers for the constraint $A-Z=0$.
Optimizing \eqref{eq:admm} then consists of iterating the following three steps until
convergence:
\begin{enumerate}
\item $\displaystyle A^{t+1} \leftarrow \argmin_A \Ell_\rho\left(A, Z^t, W^t\right) = \argmin_A \frac{1}{2}
\left\|X - \widehat{D}A\right\|^2 + \frac{\rho}{2}\left\|A - Z^t + W^t\right\|^2$,
\item $\displaystyle Z^{t+1} \leftarrow \argmin_Z \Ell_\rho\left(A^{t+1}, Z, W^t\right) =
\prox_{\rho^{-1}\Omega}\left(A^{t+1} + W^t \right)$,
\item $W^{t+1} \leftarrow W^t + A^{t+1} - Z^{t+1}$.
\end{enumerate}

The first step updates $A$ by solving a least-squares problem, and as we will show in
\Cref{sec:astep}, the sparsity structure of $\widehat{D}$ enables an extremely
efficient solution.  The second step updates $Z$ by computing the proximal mapping of
the regularization function, and in \Cref{sec:zstep}, we show how to compute this
update for time-domain regularization functions.  Finally, the third step updates the
residual error between $A$ and $Z$.

\subsection{A-step: efficient ridge regression}
\label{sec:astep}
Just as in the ADMM formulation of LASSO~\cite[chapter 6.4]{boyd2011}, the $A$ update 
takes the form of a ridge regression.  Setting the gradient to zero yields the update
equation:
\begin{align}
\zero = \nabla_A \Ell_\rho(A, Z^t, W^t) &= -\widehat{D}\trans \left(X - \widehat{D}A\right)
+ \rho(A - Z^t + W^t)\notag\\
\Rightarrow \quad A^{t+1} &= {\left(\widehat{D}\trans \widehat{D} + \rho I\right)}^{-1}
\left(\widehat{D}\trans X + \rho \left(Z^t - W^t\right)\right).\label{eq:a-update}
\end{align}
Now, observe that the inverted matrix is the sum of a $2dm\times 2dm$ diagonal matrix 
$\rho I$ and product of low-rank matrices $(\widehat{D}\trans\widehat{D})$.  
Applying the matrix inversion lemma yields
\begin{equation}
{\left(\widehat{D}\trans \widehat{D} + \rho I\right)}^{-1} = \rho^{-1} I - \rho^{-2} \widehat{D}\trans
{\left(I + \rho^{-1} \widehat{D}\widehat{D}\trans \right)}^{-1}
\widehat{D}.\label{eq:inversion}
\end{equation}
This alternative formulation superficially reduces complexity to $2d\times 2d$
from the original $2dm\times 2dm$ matrix.  However, to see the full
benefits of this formulation, we will require the following results.

\begin{lemma}
Let ${u, v \in \R^{d\times m}}$, and let ${\widehat{u} \defeq
[\diag(u_1),\dots,\diag(u_m)] \in \R^{d\times dm}}$ denote the 
matrix of diagonals formed from the columns of $u$ as in \cref{lemma:hadamard} 
(and analogously for $v$).  Then ${\widehat{u}\widehat{v}\trans = I \circ (uv\trans)}$.
\label{lemma:product}
\end{lemma}
\begin{proof}
First, observe that $\widehat{u}$ and $\widehat{v}$ share the same sparsity pattern.  
By construction, any distinct pair of rows $i \neq k$ are mutually orthogonal:
$\widehat{u}_i \widehat{v}_k\trans = 0$, so the product $\widehat{u}\widehat{v}\trans$ 
is diagonal.  Finally, along the diagonal, we have
\[
{\left[\widehat{u}\widehat{v}\trans\right]}_{ii} = \sum_k \widehat{u}_{ik}
\widehat{v}_{ik} = \sum_{k} u_{ik} v_{ik} = {\left[uv\trans\right]}_{ii}.
\]
\end{proof}

\begin{theorem}
Let $\widehat{D} \in \R^{2d\times 2dm}$ be as defined in \eqref{eq:realized}.  Then
$\widehat{D}\widehat{D}\trans \in \R^{2d\times 2d}$ is diagonal.\label{thm:diagonal}
\end{theorem}
\begin{proof}
Let $U$ and $V$ denote the blocks of $\widehat{D}$ corresponding to the
real and imaginary components.  Then,
\begin{align}
\widehat{D}\widehat{D}\trans = \left[\begin{array}{rr}U & -V\\ V & U\end{array}\right]
\left[\begin{array}{rr} U\trans & V\trans\\ -V\trans & U\trans\end{array}\right]
&= \left[\begin{array}{rr} UU\trans + VV\trans & UV\trans - VU\trans \\ VU\trans -
UV\trans & UU\trans + VV\trans\end{array} \right]\notag\\
&= \left[\begin{array}{rr}UU\trans + VV\trans & 0 \\ 0 & UU\trans + VV\trans\end{array}
\right],\label{eq:dictpower}
\end{align}
where the last equality follows from \cref{lemma:product}, which implies 
$UV\trans = VU\trans$.  Also by \cref{lemma:product}, both $UU\trans$ and $VV\trans$ are 
diagonal, as is their sum.  So $\widehat{D}\widehat{D}\trans$ is diagonal as well.
\end{proof}


As a consequence of \Cref{thm:diagonal}, the $A$ update simplifies to a diagonal matrix
inversion, which can be computed in linear time --- $\Oh(d)$ to invert, $\Oh(dm)$
including the initial sum across all codewords, which can be pre-computed and 
cached --- followed by sparse matrix multiplications.
This contrasts with the analogous result for general ridge regression problems, where the 
matrix inversion lemma can reduce complexity to quadratic, not linear, in the smaller 
dimension. 
Additionally, the precise form of $\widehat{D}\widehat{D}\trans$ in \eqref{eq:dictpower} 
carries an intuitive interpretation in \eqref{eq:inversion} as normalizing each frequency 
bin by the total power (magnitude-squared) contained in that frequency across all 
codewords.


\subsection{Z-step: regularization in the time domain}
\label{sec:zstep}
The $Z$ update is achieved by applying the proximal mapping of the (scaled) 
regularization function $\rho^{-1}\Omega$ to $A^{t+1} + W^t$.  
Given a convex function $\omega$ in the time domain, we generally define its
frequency-domain counterpart as 
$\Omega(A) \defeq \omega\left(\F^{-1}\{A\}\right) = \omega\left(\left\{\F^{-1}\{A_k\}\right\}\right)$.  By linearity of
$\F^{-1}$, $\Omega$ is convex in $A$, though not necessarily convenient to work with.

As a concrete example, consider the commonly used $\ell_1$ regularization:
\[
\omega(\{a_k\}) \defeq \lambda \sum_k \|a_k\|_1
\quad \Rightarrow \quad
\Omega(A) = \lambda \sum_k \| \F^{-1} \left\{ A_k \right\} \|_1,
\]
where $\lambda>0$ is a user-specified parameter which controls the balance between
sparsity of the solution and accuracy of the reconstruction.  While $\omega$ has a
well-known and easily computable proximal mapping via soft-thresholding, the case 
for $\Omega$ is less clear.

More generally, we seek a solution to the following problem for some vector $U$:
\[
Z \leftarrow \prox_{\rho^{-1}\Omega}(U) = \argmin_Z \frac{1}{\rho}\Omega(Z) + \frac{1}{2} \| Z - U\|^2.
\]
By applying Parseval's theorem, we can equivalently express this update in the time
domain as
\begin{align*}
Z &\leftarrow \argmin_Z \frac{1}{\rho} \omega(\F^{-1}(Z)) +
\frac{1}{2}\left\|\F^{-1}\{Z - U\}\right\|^2\notag\\
&= \F\left\{\argmin_z \frac{1}{\rho} \omega(z) + \frac{1}{2} \|z - u\|^2
\right\},\label{eq:time-regularize}
\end{align*}
where the (inverse) Fourier transforms are taken to be applied independently to 
each codeword sub-vector $Z_k$.  
This motivates the following strategy for the $Z$-step:
\begin{equation}
Z^{t+1} \leftarrow \F\left\{ \prox_{\rho^{-1}\omega} \F^{-1}\left\{A^{t+1} + W^t
\right\} \right\}.\label{eq:timeprox}
\end{equation}
That is, first convert $A^{t+1} + W^t$ to the time domain, apply the
proximal mapping, and then transform the result back into the frequency domain.

It is worth noting that \eqref{eq:timeprox} would incur significant cost if applied 
in a proximal or projected gradient descent solver: two additional Fourier transforms 
for each activation function at each step.  However, in the ADMM setting, these updates 
happen much less frequently, and in practice, typically a few dozen updates suffice to 
reach acceptable precision.


\section{Dictionary learning}

\section{Experiments}
\section{Conclusion}

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
