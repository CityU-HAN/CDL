\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage[noabbrev]{cleveref}
\usepackage{url}
\usepackage{brian}
\usepackage{booktabs}
\usepackage{graphicx}
% \usepackage{microtype}
\usepackage[sort]{natbib}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09


\title{Efficient convolutional coding and dictionary learning}


\author{
Brian McFee\\
Center for Jazz Studies\\
Columbia University\\
New York, NY 10027 \\
\texttt{brm2132@columbia.edu}
\And
Daniel P.W. Ellis\\
Department of Electrical Engineering\\
Columbia University\\
New York, NY 10027\\
\texttt{dpwe@columbia.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

% \nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
We make convolutional coding faster.
\end{abstract}

\section{Introduction}
\cite{mairal2010}
% TODO:   2013-05-24 18:26:56 by Brian McFee <brm2132@columbia.edu>
% why convolutional coding?
%   shift-invariance, robustness against windowing/offset effects
%   fixed codebooks tend to be overcomplete, learn the same basis at multiple shifts
%       which is wasteful
%   convolutional coding lets us use a smaller codebook
%   positions of activations can be semantically meaningful, eg in music transcription
%       cite klapuri2012


\subsection{Related work}
\cite{grosse2007}

\subsection{Preliminaries}
$\F:\R^d\rightarrow\C^d$ will denote the discrete Fourier transform, and $*$ will denote
convolution.
Time-domain signals will be represented by lower-case letters, \eg, $x \in \R^d$, while
frequency-domain representations will be denoted by capitals, $\F\{x\} = X \in \C^d$.
For a vector $u$, $\diag(u)$ will denote the square, diagonal matrix such that 
${\diag(u)}_{ii} = u_i$.  Unless otherwise stated, $\|\cdot\|$ will indicate the
$\ell_2$-norm.  The imaginary unit will be denoted by $j \defeq \sqrt{-1}$.

\section{Convolutional coding}
\label{sec:convcode}
Given a set of $m$ codewords ${\{d_k\}}_{k=1}^m \subset \R^d$ and a signal $x\in\R^d$, 
we seek a set of activation functions ${\{a_k\}}_{k=1}^m\subset\R^d$ which
can be used to reconstruct $x$ by convolving with the codewords:
\begin{align}
x &\approx \sum_{k=1}^m d_k * a_k.\label{eq:convapprox}
\end{align}
Because this parameterization increases the dimensionality of the representation from $d$ 
to $md$, we also desire that each activation function $a_k$ be sparse.  
Formally, this is accomplished by solving the following ``shift-invariant sparse
coding'' (SISC) optimization problem:
\begin{align}
\text{(SISC)}\quad\quad \min_{\{a_k\}} \frac{1}{2} \left\|x - \sum_{k=1}^m d_k * a_k\right\|^2 +
g(\{a_k\}), \label{eq:sisc}
\end{align}
where $g$ is a convex function subsuming any sparsity-promoting regularization 
(\eg, the $\ell_1$-norm) or additional constraints (\eg, non-negativity of activations).

Because convolution is a linear operation, \eqref{eq:sisc} is convex in $a_k$.  
However, direct optimization is difficult.\bn{MORE}

\subsection{Coding in the frequency domain}
By applying Parseval's theorem and the convolution theorem,\footnote{We assume circular
convolutions throughout this article. However, linear convolutions can be accommodated
in the usual way by appropriate padding and truncation. Similarly, for simplicity, all 
convolutions are assumed to be one-dimensional, but the formulation trivially
generalizes to the multi-dimensional case.} we can equivalently express the 
reconstruction term of \eqref{eq:sisc} in the frequency domain:\footnote{To ease
presentation, we assume the symmetrically normalized form of the Fourier transform, 
so that $\|x\| = \|\F\{x\}\|$. The normalization $1/\sqrt{d}$ may be included
explicitly, or absorbed into $G$.}
\begin{align}
\left\|x - \sum_k d_k * a_k\right\|^2 &= \left\|\F\left\{x - \sum_k d_k *
a_k\right\}\right\|^2
= \left\| X - \sum_k D_k \circ A_k\right\|^2,\label{eq:fourier}
\end{align}
where $X \defeq \F\{x\}$, $D_k \defeq \F\{d_k\}$, $A_k \defeq \F\{a_k\}$,
and $\circ$ denotes the Hadamard product: ${{[u\circ v]}_i \defeq u_i v_i}$.
While the right-hand side of \eqref{eq:fourier} nearly resembles a standard
least-squares objective, it can be put into a more familiar form via the following
lemma.
\begin{lemma}
Let $\{D_1, D_2, \dots, D_m\}, \{A_1, A_2, \dots, A_m\} \subset \C^d$.  
Then there exist ${\widehat{D} \in \C^{d\times dm}}$ and ${\vec{A} \in \C^{dm}}$ such that 
${\widehat{D}\vec{A} = \sum_{k=1}^m D_k \circ A_k}$.\label{lemma:hadamard}
\end{lemma}
\begin{proof}
$D_k$ and $A_k$ can be rearranged as follows:
\begin{align*}
\widehat{D} &\defeq \left[\diag(D_1),\; \diag(D_2),\; \dots,\; \diag(D_m)\right] \in \C^{d\times dm}\\
\vec{A} &\defeq \left[A_1\trans, A_2\trans, \dots,
A_m\trans\right]\trans \in \C^{dm}\\
\Rightarrow \quad {\left[\widehat{D}\vec{A}\right]}_i &= \sum_\ell \widehat{D}_{i\ell}
\vec{A}_\ell = \sum_k {[D_k]}_i {[A_k]}_i = \sum_k
{[D_k \circ A_k]}_i.
\end{align*}
\end{proof}

\Cref{lemma:hadamard} allows us to interchange the Hadamard product in
\eqref{eq:fourier} with a dot-product, so that 
$\left\|x - \sum_k d_k * a_k\right\|^2 =  \left\|X - \widehat{D}\vec{A}\right\|^2$,
but we are left with a least-squares objective function parameterized by the complex-valued 
vector $\vec{A}$. To facilitate the use of standard optimization techniques to solve for
$\vec{A}$, we separate the real and imaginary components as follows:
\begin{equation}
X \mapsto \left[\begin{array}{r}\Re X \\ \Im X\end{array}\right] \in \R^{2d},
\quad\quad \widehat{D} \mapsto \left[\begin{array}{rr}\Re \widehat{D} & - \Im \widehat{D}\\
\Im \widehat{D} & \Re \widehat{D}\end{array}\right] \in \R^{2d\times 2dm},
\quad\quad \vec{A} \mapsto \left[\begin{array}{r}\Re \vec{A} \\ \Im \vec{A}\end{array}\right]
\in \R^{2dm}.\label{eq:realized}
\end{equation}
With a slight abuse of notation, we can equivalently express \eqref{eq:sisc} as
\begin{equation}
\min_{A \in \R^{2dm}} \left\|X - \widehat{D}A\right\|^2 + G(A),\label{eq:realvalued}
\end{equation}
where $A_k$ corresponds to the sub-vector of $A$ associated with the $k$th codeword, and 
$G$ is a frequency-domain parameterization of the regularization function $g$.
Given an optimal $A$, each activation function $a_k$ can be 
recovered by the inverse Fourier transform of the combined real and imaginary components
corresponding to $A_k$:
\begin{equation}
a_k \leftarrow \F^{-1} \left\{ {\left[[I,\; jI]A \right]}_k
\right\}.\label{eq:complexify}
\end{equation}

\subsection{ADMM solver for encoding}
\label{sec:solver}
At a first glance, \eqref{eq:realvalued} may seem no easier to solve than the original
formulation of \eqref{eq:sisc}, especially since in many cases, $G$ may be most
naturally parameterized in the time domain.  We therefore apply the alternating
direction method of multipliers (ADMM) technique to separate the regularization and
reconstruction terms~\cite{boyd2011}.  This leads to the following equivalent
formulation:
\begin{equation}
\min_{A, Z} \frac{1}{2}\left\|X - \widehat{D}A\right\|^2 + G(Z)
\quad\quad \suchthat\quad A - Z = 0.\label{eq:admm}
\end{equation}

Forming the (scaled-form) augmented Lagrangian for \eqref{eq:admm} with scaling
parameter $\rho > 0 $ yields
\[
\Ell_\rho(A,Z,W) = \frac{1}{2}\left\|X - \widehat{D}A\right\|^2 + G(Z) +
\frac{\rho}{2}\|A - Z + W\|^2,
\]
where $W$ is a vector of (scaled) Lagrange multipliers for the constraint $A-Z=0$.
Optimizing \eqref{eq:admm} then consists of iterating the following three steps until
convergence:
\begin{enumerate}
\item $\displaystyle A^{t+1} \leftarrow \argmin_A \Ell_\rho\left(A, Z^t, W^t\right) = \argmin_A \frac{1}{2}
\left\|X - \widehat{D}A\right\|^2 + \frac{\rho}{2}\left\|A - Z^t + W^t\right\|^2$,
\item $\displaystyle Z^{t+1} \leftarrow \argmin_Z \Ell_\rho\left(A^{t+1}, Z, W^t\right) =
\prox_{\rho^{-1}G}\left(A^{t+1} + W^t \right)$,
\item $W^{t+1} \leftarrow W^t + A^{t+1} - Z^{t+1}$.
\end{enumerate}

The first step updates $A$ by solving a least-squares problem, and as we will show in
\Cref{sec:astep}, the sparsity structure of $\widehat{D}$ enables an extremely
efficient solution.  The second step updates $Z$ by computing the proximal mapping of
the regularization function, and in \Cref{sec:zstep}, we show how to compute this
update for time-domain regularization functions.  Finally, the third step updates the
residual error between $A$ and $Z$.

\subsection{A-step: efficient ridge regression}
\label{sec:astep}
Just as in the ADMM formulation of LASSO~\cite[chapter 6.4]{boyd2011}, the $A$ update 
takes the form of a ridge regression.  Setting the gradient to zero yields the update
equation:
\begin{align}
\zero = \nabla_A \Ell_\rho(A, Z^t, W^t) &= -\widehat{D}\trans \left(X - \widehat{D}A\right)
+ \rho(A - Z^t + W^t)\notag\\
\Rightarrow \quad A^{t+1} &= {\left(\widehat{D}\trans \widehat{D} + \rho I\right)}^{-1}
\left(\widehat{D}\trans X + \rho \left(Z^t - W^t\right)\right).\label{eq:a-update}
\end{align}
Now, observe that the inverted matrix is the sum of a $2dm\times 2dm$ diagonal matrix 
$\rho I$ and product of low-rank matrices $(\widehat{D}\trans\widehat{D})$.  
Applying the matrix inversion lemma yields
\begin{equation}
{\left(\widehat{D}\trans \widehat{D} + \rho I\right)}^{-1} = \frac{1}{\rho}\left( I -  \widehat{D}\trans
{\left(\widehat{D}\widehat{D}\trans  + \rho I \right)}^{-1}
\widehat{D}\right).\label{eq:inversion}
\end{equation}
This alternative formulation superficially reduces the inversion complexity to 
$2d\times 2d$ from the original $2dm\times 2dm$ matrix.  However, to see the full
benefits of this formulation, we will require the following results.

\begin{lemma}
Let ${u, v \in \R^{d\times m}}$, and let ${\widehat{u} \defeq
[\diag(u_1),\dots,\diag(u_m)] \in \R^{d\times dm}}$ denote the 
matrix of diagonals formed from the columns of $u$ as in \cref{lemma:hadamard} 
(and analogously for $v$).  Then ${\widehat{u}\widehat{v}\trans = I \circ (uv\trans)}$.
\label{lemma:product}
\end{lemma}
\begin{proof}
First, observe that $\widehat{u}$ and $\widehat{v}$ share the same sparsity pattern.  
By construction, any distinct pair of rows $i \neq k$ are mutually orthogonal:
$\widehat{u}_i \widehat{v}_k\trans = 0$, so the product $\widehat{u}\widehat{v}\trans$ 
is diagonal.  Finally, along the diagonal, we have
\[
{\left[\widehat{u}\widehat{v}\trans\right]}_{ii} = \sum_\ell \widehat{u}_{i\ell}
\widehat{v}_{i\ell} = \sum_{k} u_{ik} v_{ik} = {\left[uv\trans\right]}_{ii}.
\]
\end{proof}

\begin{theorem}
Let $\widehat{D} \in \R^{2d\times 2dm}$ be as defined in \eqref{eq:realized}.  Then
$\widehat{D}\widehat{D}\trans \in \R^{2d\times 2d}$ is diagonal.\label{thm:diagonal}
\end{theorem}
\begin{proof}
Let $U$ and $V$ denote the blocks of $\widehat{D}$ corresponding to the
real and imaginary components.  Then,
\begin{align}
\widehat{D}\widehat{D}\trans = \left[\begin{array}{rr}U & -V\\ V & U\end{array}\right]
\left[\begin{array}{rr} U\trans & V\trans\\ -V\trans & U\trans\end{array}\right]
% &= \left[\begin{array}{rr} UU\trans + VV\trans & UV\trans - VU\trans \\ VU\trans -
% UV\trans & UU\trans + VV\trans\end{array} \right]\notag\\
&= \left[\begin{array}{rr}UU\trans + VV\trans & 0 \\ 0 & UU\trans + VV\trans\end{array}
\right],\label{eq:dictpower}
\end{align}
where the last equality follows by symmetry of $\widehat{D}\widehat{D}\trans$, which 
implies $UV\trans = VU\trans$.  By \cref{lemma:product}, both $UU\trans$ and $VV\trans$ 
are diagonal, as is their sum.  So $\widehat{D}\widehat{D}\trans$ is diagonal as well.
\end{proof}


As a consequence of \Cref{thm:diagonal}, the $A$ update simplifies to a diagonal matrix
inversion, which can be computed in linear time --- $\Oh(d)$ to invert, $\Oh(dm)$
including the initial sum across all codewords, which can be pre-computed and 
cached --- followed by sparse matrix multiplications.
This contrasts with the analogous result for general ridge regression problems, where the 
matrix inversion lemma can reduce complexity to quadratic, not linear, in the smaller 
dimension. 
Additionally, the precise form of $\widehat{D}\widehat{D}\trans$ in \eqref{eq:dictpower} 
carries an intuitive interpretation in \eqref{eq:inversion} as normalizing each frequency 
bin by the total power (magnitude-squared) contained in that frequency across all 
codewords.


\subsection{Z-step: regularization in the time domain}
\label{sec:zstep}
The $Z$ update is achieved by applying the proximal mapping of the (scaled) 
regularization function $\rho^{-1}G$ to $A^{t+1} + W^t$.  
Given a convex function $g$ in the time domain, we generally define its
frequency-domain counterpart as 
$G(A) \defeq g\left(\F^{-1}\{A\}\right) = g\left(\left\{\F^{-1}\{A_k\}\right\}\right)$.  By linearity of
$\F^{-1}$, $G$ is convex in $A$, though not necessarily convenient to work with.

As a concrete example, consider the commonly used $\ell_1$ regularization:
\[
g(\{a_k\}) \defeq \lambda \sum_k \|a_k\|_1
\quad \Rightarrow \quad
G(A) = \lambda \sum_k \| \F^{-1} \left\{ A_k \right\} \|_1,
\]
where $\lambda>0$ is a user-specified parameter which controls the balance between
sparsity of the solution and accuracy of the reconstruction.  In this case, $g$ has a
well-known and easily computable proximal mapping via coordinate-wise soft-thresholding:
\[
{\left[\prox_{\rho^{-1} g}(z)\right]}_i = {\left[S_{\lambda/\rho}(z)\right]}_i \defeq \begin{cases}
z_i - \lambda / \rho & z_i \geq \lambda / \rho\\
z_i + \lambda / \rho & z_i \leq -\lambda / \rho\\
0 & \text{otherwise}
\end{cases},
\]
but the case for $G$ is less clear.

More generally, we seek a solution to the following problem for $U = A + W$:
\[
Z \leftarrow \prox_{\rho^{-1}G}(U) = \argmin_Z \frac{1}{\rho}G(Z) + \frac{1}{2} \| Z - U\|^2.
\]
By applying Parseval's theorem, we can equivalently express this update in the time
domain as
\begin{align}
Z &\leftarrow \argmin_Z \frac{1}{\rho} g(\F^{-1}(Z)) + \frac{1}{2}\left\|\F^{-1}\{Z -
U\}\right\|^2\notag\\
&= \F\left\{\argmin_z \frac{1}{\rho} g(z) + \frac{1}{2} \|z - u\|^2
\right\}
=\F\left\{ \prox_{\rho^{-1}g} \F^{-1}\left\{A^{t+1} + W^t
\right\} \right\},\label{eq:timeprox}
\end{align}
where the (inverse) Fourier transforms are applied separately to each sub-vector $Z_k$.  

It is worth noting that \eqref{eq:timeprox} would incur significant cost if applied 
in a proximal gradient descent optimizer: two additional Fourier transforms 
for each activation function at each step.  However, in the ADMM setting, these updates 
happen much less frequently, and in practice, typically a few dozen updates suffice to 
reach acceptable precision.

\subsection{Encoding algorithm}
\label{sec:algorithm}

\begin{algorithm}[t]
\caption{ADMM Shift-invariant sparse coding}
\label{alg:sisc}
\begin{algorithmic}[1]
\REQUIRE{Signal $x \in \R^d$, codewords $\{d_1, d_2, \dots, d_m\} \subset \R^d$,
sparsity parameter $\lambda > 0$}
\ENSURE{Activations $\{a_1, a_2, \dots, a_m\} \subset \R^d$}
\STATE{Compute $X$ and $\widehat{D}$ via \eqref{eq:realized}}
\COMMENT{represent signals in the frequency domain}

\STATE{$R \leftarrow {\left(\widehat{D}\trans\widehat{D} + \rho I\right)}^{-1}$}
\COMMENT{compute efficiently by \eqref{eq:inversion}}

\STATE{Initialize $Z \leftarrow 0, \quad W \leftarrow 0$}
\REPEAT{}
    \STATE{$A \leftarrow R \left(\widehat{D}\trans X + \rho (Z - W)\right)$}
    \COMMENT{update activations}

    \STATE{$Z \leftarrow \F\left\{S_{\lambda/\rho} \left( \F^{-1}\left\{A + W\right\}
    \right) \right\}$} 
    \COMMENT{time-domain soft-thresholding}
    
    \STATE{$W \leftarrow W + A - Z$} 
    \COMMENT{update residual vector}
    
    \STATE{(optional) Update $\rho$ and recompute $R$}
\UNTIL{convergence}

\RETURN{${\{a_k\}}_{k=1}^m$} 
\COMMENT{compute by \eqref{eq:complexify}}
\end{algorithmic}
\end{algorithm}

By combining the $A$- and $Z$-step from the previous sections, we arrive at the
$\ell_1$ sparse coding algorithm listed as \Cref{alg:sisc}.  \Cref{alg:sisc} follows
the general ADMM pattern, including an optional step (line 8) in which the scaling 
parameter $\rho$ is updated (typically increased) after each iteration~\cite{boyd2011}.
Changing $\rho$ necessitates that we re-compute the inverse
${\widehat{D}\trans\widehat{D} + \rho I}$, but due to \Cref{thm:diagonal}, the
additional computational overhead is subsumed by the sparse matrix multiplication 
in step 5.  In our implementation, $\rho$ is only changed when the ratio between primal 
and dual residual norms exceeds a fixed constant (\eg, 10), so the number of inversions
is typically much smaller than the total number of iterations.

\section{Dictionary learning}
\label{sec:dict}
Given a sample of signals $\{x_1, x_2, \dots, x_n\} \subset \R^d$, the dictionary
learning task is to find codewords $\{d_1, d_2, \dots, d_m\}$ which optimize the sparse
reconstruction objective of \eqref{eq:sisc}:
\begin{equation}
\min_{\{d_k\}}\;\frac{1}{n} \sum_{i=1}^n \min_{\{a^i_k\}} \frac{1}{2} \left\|x_i - \sum_k d_k
* a^i_k\right\|^2 + g(\{a^i_k\}) + h(\{d_k\}),\label{eq:dict}
\end{equation}
where $h(\cdot)$ encodes feasibility constraints on the codewords (\eg, bounded
$\ell_2$-norm), and $a_k^i$ denotes the activation of the
$k$th codeword for the $i$th data point.  
Since \eqref{eq:dict} is not jointly convex in $d$ and $a$, a standard technique is to
alternate between optimizing the activations ($a$) with a fixed dictionary, and then
optimizing the codewords ($d$) with the activations fixed.

For a fixed set of activations, we can apply the same general technique as used in
\Cref{sec:convcode} to derive an optimization problem over $D_k$ in the frequency
domain.  By commutativity of the Hadamard product, we can swap the roles of $D$ and $A$
in \cref{lemma:hadamard}.  Subsequently separating the real and imaginary components as
in \eqref{eq:realized} yields following representations:
\begin{align}
\widehat{A}^i \defeq \left[ \diag(A^i_1),\; \diag(A^i_2),\;, \dots,
\diag(A^i_m)\right] &\mapsto \left[\begin{array}{rr}
\Re \widehat{A}^i & -\Im \widehat{A}^i\\
\Im \widehat{A}^i & \Re\widehat{A}^i\end{array} \right] \in \R^{2d\times
2dm}\notag
\\
\vec{D} \defeq \left[D_1\trans,\; D_2\trans, \; \dots, \; D_m\trans\right]\trans
&\mapsto \left[\begin{array}{r}\Re \vec{D}\\ \Im\vec{D}\end{array} \right]\in \R^{2dm}.
\label{eq:realized-dict}
\end{align}
We can then form the frequency-domain optimization problem for $D$:
\begin{equation}
\min_{D \in \R^{2dm}}\; \frac{1}{n}\sum_{i=1}^n \frac{1}{2} \left\|X^i - \widehat{A}^i D
\right\|^2 + H(D),\label{eq:freqdict}
\end{equation}
where $H$ is the frequency-domain parameterization of $h$.

\subsection{ADMM solver for dictionary learning}
The usual dictionary learning formulation defines $h$ to apply an $\ell_2$ constraint to 
each codeword, \ie: 
\[
h(\{d_k\}) = \begin{cases}
0 & \forall k: \; d_k\trans d_k \leq 1\\
\infty & \text{otherwise}
\end{cases},
\]
which translates naturally to
the frequency domain: $D_k\trans D_k \leq 1$.  The resulting quadratically-constrained
quadratic program can be solved in the dual by Newton's method, as done by Grosse
\etal; however, the resulting updates involve computing many matrix 
inversions at each step, which can be slow in practice~\cite[footnote 10]{grosse2007}.

Instead, we will derive an ADMM algorithm to optimize \eqref{eq:freqdict}. As in
\Cref{sec:solver}, we form an equivalent program:
\begin{equation}
\min_{D,E}\; \frac{1}{n} \sum_{i=1}^n \frac{1}{2} \left\|X^i - \widehat{A}^i D\right\|^2 +
H(E)\quad\quad\suchthat\quad D-E=0.\label{eq:freqdict:admm}
\end{equation}
The augmented Lagrangian for \eqref{eq:freqdict:admm} is then:
\[
\Ell_\rho(D, E, F) = \frac{1}{n} \sum_{i=1}^n \left\|X^i - \widehat{A}^i D\right\|^2 + H(E) +
\frac{\rho}{2}\left\|D - E + F\right\|^2,
\]
where $F$ denotes the (scaled) Lagrangian parameters.  Again, we solve
\eqref{eq:freqdict:admm} by iteratively updating $D,E$, and $F$ until convergence:
\begin{enumerate}
\item $\displaystyle D^{t+1} \leftarrow \argmin_D \Ell_\rho(D, E^t, F^t)$,
\item $\displaystyle E^{t+1} \leftarrow \argmin_E \Ell_\rho(D^{t+1}, E, F^t)$,
\item $F^{t+1} \leftarrow F^t + D^{t+1} - E^{t+1}$.
\end{enumerate}
The $E$-step consists of projecting each sub-vector of $D^{t+1} + F^t$ onto the
unit $\ell_2$-ball.  

The $D$ update consists of minimizing $\Ell_\rho$ for fixed $E$ and $F$, which is
achieved by
\begin{align}
\nabla_D \Ell_\rho(D, E^t, F^t) &= \frac{1}{n} \sum_{i=1}^n
-{\left(\widehat{A}^i\right)}\trans (X_i - \widehat{A}^i D) + \rho\left(D - E^t +
F^t\right) = \zero\notag\\
\Rightarrow \quad D^{t+1} &= {\left(\rho I + \frac{1}{n} \sum_{i=1}^n
\left(\widehat{A}^i\right)\trans \widehat{A}^i \right)}^{-1} 
\left(\frac{1}{n}\sum_{i=1}^n  \left(\widehat{A}^i\right)\trans X_i + \rho\left(E^t -
F^t\right)\right).\label{eq:d-update}
\end{align}
While superficially similar to \eqref{eq:a-update}, the matrix inversion lemma does not
apply to \eqref{eq:d-update} due to the sum over $n$ matrices.  Due to the common
sparsity structure of each $\widehat{A}^i$, the resulting sum also exhibits a highly 
regular sparsity structure, as noted below.

\begin{proposition}
Let $\left\{\widehat{A}^i\right\}$ be of the form in~\eqref{eq:realized-dict}.  
Then there exists a permutation matrix $\Pi$ such that 
$\Pi\left(\sum_i \left(\widehat{A}^i\right)\trans \widehat{A}^i \right)\Pi\trans$ 
is block-diagonal with blocks of size $2m\times 2m$.\label{prop:permute}
\end{proposition}

As a result of \cref{prop:permute}, \eqref{eq:d-update} can be efficiently computed by
a sparse LU-decomposition, which amounts to computing $d$ dense $2m\times2m$
LU-decompositions in time $\Oh(dm^3)$ rather than the $\Oh(d^3m^3)$ work required by 
na\"{\i}vely ignoring the sparsity structure.

While this approach may not appear to buy much over the dual formulation of Grosse~\etal, 
a key distinction is that here the inversions (LU-decompositions) are computed
infrequently, only when the scaling parameter $\rho$ changes.  The resulting
dictionary-learning algorithm is listed as \Cref{alg:dict}.

\begin{algorithm}[t]
\caption{ADMM Convolutional dictionary learning}
\label{alg:dict}
\begin{algorithmic}[1]
\REQUIRE{Data ${\{x_i\}} \subset \R^d$, activations ${\{a^i_k\}} \subset \R^{d\times m}$}
\ENSURE{Codewords ${\{d_k\}}_{k=1}^m \subset \R^d$}
\STATE{Compute $X^i$ and $\widehat{A}^i$ via \eqref{eq:realized-dict}}
\COMMENT{transform to frequency domain}

\STATE{$S \leftarrow \frac{1}{n} \sum_{i=1}^n \left(\widehat{A}^i\right)\trans \widehat{A}^i$}
\COMMENT{cache data-codeword interactions}
\STATE{$Y \leftarrow \frac{1}{n} \sum_{i=1}^n \left(\widehat{A}^i\right)\trans X_i$}
\STATE{Initialize $E \leftarrow 0,\quad F\leftarrow 0$}
\REPEAT{}
    \STATE{$D \leftarrow {\left(\rho I + S \right)}^{-1} \left(Y + \rho(E - F) \right)$}
    \COMMENT{update codewords}

    \STATE{$\displaystyle \forall k: \; E_k \leftarrow \proj_{\ell_2 \leq 1} \left( D_k + F_k \right)$}
    \COMMENT{project onto the feasible set}

    \STATE{$F \leftarrow F + D - E$}
    \COMMENT{update residuals}

    \STATE{(optional) Update $\rho$ and re-compute ${\left(\rho I + S\right)}^{-1}$}
    \COMMENT{sparse LU-decomposition}
\UNTIL{convergence}
\RETURN{ ${\{d_k\}}_{k=1}^m$}
\end{algorithmic}
\end{algorithm}


\section{Experiments}

\section{Conclusion}

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
